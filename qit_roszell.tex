\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\geometry{letterpaper}   


\usepackage{amssymb,amsfonts,amsmath,bbm,mathrsfs,stmaryrd, mathtools}
\usepackage{xcolor}
\usepackage{url}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usetikzlibrary{cd}

%\usepackage{parskip}

\usepackage[colorlinks,
             linkcolor=black!75!red,
             citecolor=blue,
             pdftitle={},
             pdfauthor={},
             pdfproducer={pdfLaTeX},
             pdfpagemode=None,
             bookmarksopen=true
             bookmarksnumbered=true]{hyperref}

\usepackage{tikz}
\usetikzlibrary{cd,arrows,calc,decorations.pathreplacing,decorations.markings,intersections,shapes.geometric,through,fit,shapes.symbols,positioning,decorations.pathmorphing}

\usepackage{braket}

\renewcommand{\theequation}{\thesection.\arabic{equation}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Theorems and references %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[amsmath,thmmarks,hyperref]{ntheorem}
\usepackage{cleveref}

\creflabelformat{enumi}{#2(#1)#3}

\crefname{section}{Section}{Sections}
\crefformat{section}{#2Section~#1#3} 
\Crefformat{section}{#2Section~#1#3} 

\crefname{subsection}{\S}{\S\S}
\crefformat{subsection}{#2\S#1#3} 
\Crefformat{subsection}{#2\S#1#3} 

\theoremstyle{plain}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{conjecture}[lemma]{Conjecture}
\newtheorem{question}[lemma]{Question}
\newtheorem{assumption}[lemma]{Assumption}


\theoremstyle{nonumberplain}
\newtheorem{theoremN}{Theorem}
\newtheorem{propositionN}{Proposition}
\newtheorem{corollaryN}{Corollary}


\theoremstyle{plain}
\theorembodyfont{\upshape}
\theoremsymbol{\ensuremath{\blacklozenge}}

\newtheorem{definition}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{convention}[lemma]{Convention}
\newtheorem{exercise}[lemma]{Exercise}

\crefname{definition}{definition}{definitions}
\crefformat{definition}{#2definition~#1#3} 
\Crefformat{definition}{#2Definition~#1#3} 

\crefname{ex}{example}{examples}
\crefformat{example}{#2example~#1#3} 
\Crefformat{example}{#2Example~#1#3} 

\crefname{remark}{remark}{remarks}
\crefformat{remark}{#2remark~#1#3} 
\Crefformat{remark}{#2Remark~#1#3} 

\crefname{convention}{convention}{conventions}
\crefformat{convention}{#2convention~#1#3} 
\Crefformat{convention}{#2Convention~#1#3} 

\crefname{exercise}{exercise}{exercises}
\crefformat{exercise}{#2exercise~#1#3} 
\Crefformat{exercise}{#2Exercise~#1#3} 



\crefname{lemma}{lemma}{lemmas}
\crefformat{lemma}{#2lemma~#1#3} 
\Crefformat{lemma}{#2Lemma~#1#3} 

\crefname{proposition}{proposition}{propositions}
\crefformat{proposition}{#2proposition~#1#3} 
\Crefformat{proposition}{#2Proposition~#1#3} 

\crefname{corollary}{corollary}{corollaries}
\crefformat{corollary}{#2corollary~#1#3} 
\Crefformat{corollary}{#2Corollary~#1#3} 

\crefname{theorem}{theorem}{theorems}
\crefformat{theorem}{#2theorem~#1#3} 
\Crefformat{theorem}{#2Theorem~#1#3} 

\crefname{assumption}{assumption}{Assumptions}
\crefformat{assumption}{#2assumption~#1#3} 
\Crefformat{assumption}{#2Assumption~#1#3} 

\crefname{equation}{}{}
\crefformat{equation}{(#2#1#3)} 
\Crefformat{equation}{(#2#1#3)}

\theoremstyle{nonumberplain}
\theoremsymbol{\ensuremath{\blacksquare}}

\newtheorem{proof}{Proof.}
\newtheorem{solution}{Solution.}
\newcommand\pf[1]{\newtheorem{#1}{Proof of \Cref{#1}.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% simple math operators %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\End}{\mathrm{End}}
\DeclareMathOperator{\pr}{\mathrm{Prob}}
\DeclareMathOperator{\orb}{\mathrm{Orb}}


\DeclareMathOperator{\cact}{\cat{CAct}}
\DeclareMathOperator{\wact}{\cat{WAct}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% align* numbering %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% user macros
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% below are many macros
% be careful...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% misc (should not need to touch) %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\ol{\overline}
\newcommand\wt{\widetilde}

%
\newcommand{\define}[1]{{\em #1}}
\newcommand\1{{\bf 1}}
\newcommand{\cat}[1]{\textsc{#1}}
\newcommand\mathify[2]{\newcommand{#1}{\cat{#2}}}
\newcommand\spr[1]{\cite[\href{https://stacks.math.columbia.edu/tag/#1}{Tag {#1}}]{stacks-project}}
\newcommand{\qedhere}{\mbox{}\hfill\ensuremath{\blacksquare}}


\renewcommand{\square}{\mathrel{\Box}}
\newcommand\Section[1]{\section{#1}\setcounter{lemma}{0}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% math fonts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% math blackboard font

\newcommand\bb[1]{{\mathbb #1}} 

\newcommand\bA{{\mathbb A}}
\newcommand\bB{{\mathbb B}}
\newcommand\bC{{\mathbb C}}
\newcommand\bD{{\mathbb D}}
\newcommand\bE{{\mathbb E}}
\newcommand\bF{{\mathbb F}}
\newcommand\bG{{\mathbb G}}
\newcommand\bH{{\mathbb H}}
\newcommand\bI{{\mathbb I}}
\newcommand\bJ{{\mathbb J}}
\newcommand\bK{{\mathbb K}}
\newcommand\bL{{\mathbb L}}
\newcommand\bM{{\mathbb M}}
\newcommand\bN{{\mathbb N}}
\newcommand\bO{{\mathbb O}}
\newcommand\bP{{\mathbb P}}
\newcommand\bQ{{\mathbb Q}}
\newcommand\bR{{\mathbb R}}
\newcommand\bS{{\mathbb S}}
\newcommand\bT{{\mathbb T}}
\newcommand\bU{{\mathbb U}}
\newcommand\bV{{\mathbb V}}
\newcommand\bW{{\mathbb W}}
\newcommand\bX{{\mathbb X}}
\newcommand\bY{{\mathbb Y}}
\newcommand\bZ{{\mathbb Z}}

% math script font

\newcommand\cA{{\mathcal A}}
\newcommand\cB{{\mathcal B}}
\newcommand\cC{{\mathcal C}}
\newcommand\cD{{\mathcal D}}
\newcommand\cE{{\mathcal E}}
\newcommand\cF{{\mathcal F}}
\newcommand\cG{{\mathcal G}}
\newcommand\cH{{\mathcal H}}
\newcommand\cI{{\mathcal I}}
\newcommand\cJ{{\mathcal J}}
\newcommand\cK{{\mathcal K}}
\newcommand\cL{{\mathcal L}}
\newcommand\cM{{\mathcal M}}
\newcommand\cN{{\mathcal N}}
\newcommand\cO{{\mathcal O}}
\newcommand\cP{{\mathcal P}}
\newcommand\cQ{{\mathcal Q}}
\newcommand\cR{{\mathcal R}}
\newcommand\cS{{\mathcal S}}
\newcommand\cT{{\mathcal T}}
\newcommand\cU{{\mathcal U}}
\newcommand\cV{{\mathcal V}}
\newcommand\cW{{\mathcal W}}
\newcommand\cX{{\mathcal X}}
\newcommand\cY{{\mathcal Y}}
\newcommand\cZ{{\mathcal Z}}

% math frak font

\newcommand\fA{{\mathfrak A}}
\newcommand\fB{{\mathfrak B}}
\newcommand\fC{{\mathfrak C}}
\newcommand\fD{{\mathfrak D}}
\newcommand\fE{{\mathfrak E}}
\newcommand\fF{{\mathfrak F}}
\newcommand\fG{{\mathfrak G}}
\newcommand\fH{{\mathfrak H}}
\newcommand\fI{{\mathfrak I}}
\newcommand\fJ{{\mathfrak J}}
\newcommand\fK{{\mathfrak K}}
\newcommand\fL{{\mathfrak L}}
\newcommand\fM{{\mathfrak M}}
\newcommand\fN{{\mathfrak N}}
\newcommand\fO{{\mathfrak O}}
\newcommand\fP{{\mathfrak P}}
\newcommand\fQ{{\mathfrak Q}}
\newcommand\fR{{\mathfrak R}}
\newcommand\fS{{\mathfrak S}}
\newcommand\fT{{\mathfrak T}}
\newcommand\fU{{\mathfrak U}}
\newcommand\fV{{\mathfrak V}}
\newcommand\fW{{\mathfrak W}}
\newcommand\fX{{\mathfrak X}}
\newcommand\fY{{\mathfrak Y}}
\newcommand\fZ{{\mathfrak Z}}

\newcommand\fa{{\mathfrak a}}
\newcommand\fb{{\mathfrak b}}
\newcommand\fc{{\mathfrak c}}
\newcommand\fd{{\mathfrak d}}
\newcommand\fe{{\mathfrak e}}
\newcommand\ff{{\mathfrak f}}
\newcommand\fg{{\mathfrak g}}
\newcommand\fh{{\mathfrak h}}
%\newcommand\fi{{\mathfrak i}}
\newcommand\fj{{\mathfrak j}}
\newcommand\fk{{\mathfrak k}}
\newcommand\fl{{\mathfrak l}}
\newcommand\fm{{\mathfrak m}}
\newcommand\fn{{\mathfrak n}}
\newcommand\fo{{\mathfrak o}}
\newcommand\fp{{\mathfrak p}}
\newcommand\fq{{\mathfrak q}}
\newcommand\fr{{\mathfrak r}}
\newcommand\fs{{\mathfrak s}}
\newcommand\ft{{\mathfrak t}}
\newcommand\fu{{\mathfrak u}}
\newcommand\fv{{\mathfrak v}}
\newcommand\fw{{\mathfrak w}}
\newcommand\fx{{\mathfrak x}}
\newcommand\fy{{\mathfrak y}}
\newcommand\fz{{\mathfrak z}}


\newcommand\fgl{\mathfrak{gl}}
\newcommand\fsl{\mathfrak{sl}}
\newcommand\fsp{\mathfrak{sp}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% QIT useful commands %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bmat}[1]{\begin{bmatrix*} #1 \end{bmatrix*}} % matrices
\newcommand{\setovecs}[1]{\lb \ket{v_1}, \ldots, \ket{v_{#1}}\rb} % set of vectors
\newcommand{\listovecs}[2]{\ket{{#1}_1}, \ldots, \ket{{#1}_{#2}}} % set list of NAMED vectors
\newcommand{\Tr}{\text{Tr}} % trace v1
\newcommand{\tr}{\text{tr}} % trace v2

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% standard mitch commands %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Standard Sets
\newcommand{\Q}{\mathbb{Q}} % rationals
\newcommand{\R}{\mathbb{R}} % reals
\newcommand{\Z}{\mathbb{Z}} % integers
\newcommand{\C}{\mathbb{C}} % complex numbers
\newcommand{\N}{\mathbb{N}} % natural numbers
\newcommand{\F}{\mathbb{F}} % arbitrary field
\newcommand{\T}{\mathbb{T}} % Unit circle
\newcommand{\D}{\mathbb{D}} % Open unit disc

%% Arrows
\newcommand{\ra}{\rightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\La}{\Leftarrow}

%% Greek
\newcommand{\al}{\alpha}
\newcommand{\ep}{\varepsilon} % epsilon
\newcommand{\es}{\varnothing} % empty set


%% Brackets
\newcommand{\<}{\left\langle} 
\renewcommand{\>}{\right\rangle}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\lv}{\left\lvert}
\newcommand{\rv}{\right\rvert}
\newcommand{\lb}{\left\{}
\newcommand{\rb}{\right\}}
\newcommand{\lan}{\left\langle}
\newcommand{\ran}{\right\rangle}

%% Algebra
\newcommand{\isom}{\cong} %Isomorphic
\newcommand{\nsub}{\trianglelefteq} %Normal Subgroup
\newcommand{\semi}{\rtimes} %Semi-Direct Product
\newcommand{\Aut}{\text{Aut}} % automorphism group
\newcommand{\op}{{\text{op}}} % opposite algebra

%% Linear Algebra
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\inp}[2]{\left\langle#1\mid #2\right\rangle} % inner product
\newcommand{\spn}[1]{\text{Span}\lp #1\rp} % span
\newcommand{\cspn}[1]{\overline{\text{Span}}\lp #1\rp} % closed span

%% Analysis
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert} % absolute value
\newcommand{\supp}[1]{\text{supp}\lp #1\rp} % support
\newcommand{\co}{\text{co}} % convex hull
\newcommand{\cl}[1]{\overline{#1}} % closure
\DeclareMathOperator*{\esssup}{ess\,sup} % essential supremum

%% Complex 
\newcommand{\Res}[2]{\text{Res}\lp #1, #2\rp} %Residue of a FUNCTION at a POINT
\newcommand{\Ind}{\text{Ind}} % index
\newcommand{\re}[1]{\text{Re}(#1)}  % real part
\newcommand{\im}[1]{\text{Im}(#1)} % complex part

%% QIT
\newcommand{\setofkets}[1]{\lb \ket{#1_1}, \ket{#1_2}, \ldots, \ket{#1_n}\rb} % Set of ket vectors


%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% start of document
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
% Title and Document Info
%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{MA 399 Intro to Quantum Information Theory}
\author{Hayden Roszell}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
No idea what is happening in this class lol do this later
\end{abstract}

\tableofcontents



\section{Intro to Linear Algebra}

Let's jump right in. This section is an abbreviation of the introduction to linear algebra session.

A vector space is a group of objects (vectors) which may be added together and multiplied by compatible scalars from $\R$ or $\C$. In this class, we primarily care about vector spaces $\C^n$ from $\C$ and $\R^n$ from $\R$. Recall that $\C$ is the scalar field of complex numbers $a+bi$, where multiplication is defined by the rule $i^2=-1$, and is equipped with:
\begin{enumerate}[label=(\alph*)]
    \item a complex conjugation operation -- $\ol{a+bi}=(a+bi)^*=a-bi$ and
    \item a size function called the \textbf{modulus} -- $\abs{a+bi}=\sqrt{a^2+b^2}.$
\end{enumerate}
Note that the modulus is similar to magnitude. \\
To work with complex numbers, it's useful to have an understanding of the basic operations.
\begin{enumerate}[label=(\alph*)]
	\item To add/subtract complex numbers, add/subtract the corresponding real/imaginary parts. For 		example -- $(a+bi)+(c+di)=(a+c)+(b+d)i$
	\item To multiply/divide complex numbers, multiply both parts of the complex number by the real 		number. For example -- $(a+bi)*(c+di)=ac+adi+bcj-bd=(ac-bd)+(ad+bc)i$. This form will be useful 		for the duration of the class.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Representing Vectors in Complex Spaces %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Representing Vectors in Complex Spaces}
As mentioned, this class primarily works in complex number spaces. For this reason, having useful tools for representing vectors in these abstract spaces is useful. Vectors are represented in \textbf{bra-ket} notation, where \textit{bra} represents a \textit{row} vector, and \textit{ket} represents a \textit{column} vector.

\begin{definition}\label{def:ket}
If $\ket{v}\in\C^n$ is a \textit{ket} vector which consists of $n$ complex numbers,
\begin{equation}
\ket{v}=\bmat{v_1 \\ v_2 \\ ... \\ v_n} \textrm{for } v_1, v_2, ..., v_n \in \C
\end{equation}
\end{definition}

\begin{definition}\label{def:bra}
If $\bra{v}\in\C^n$ is a \textit{bra} vector which consists of $n$ complex numbers,
\begin{equation}
\bra{v}=\bmat{v_1 & v_2 & ... & v_n} \textrm{for } v_1, v_2, ..., v_n \in \C
\end{equation}
\end{definition}
Note that the the integer $n$ in definitions \ref{def:ket} and \ref{def:bra} is called the \textbf{dimension} of the vector space $\C^n$.

\begin{example}
$\C^2$ is a 2-dimensional vector space over $\C$.
\begin{center}
$\alpha\bmat{1 \\ 0} + \beta\bmat{0 \\ 1}$ \\
$\bmat{\alpha \\ 0}+\bmat{0 \\ \beta}=\bmat{\alpha \\ \beta}$
Note: $\bmat{\alpha \\ \beta}$ 'fills up' $\C^2$; IE it represents all values contained within $\C^2$
\end{center}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Linear Combinations %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Linear Combinations}
A linear combination is a useful tool for this class which is the 'combination' or multiplication of each term in a set by constants, and then adding the result. The interesting property of linear combinations is that the result is still contained within the initial set.

\begin{definition}\label{def:li}
A linear combination of $\lbrace\ket{v_1}, ..., \ket{v_n}\rbrace\subset\C^n$ is a single vector in the form $\lambda_1\ket{v_1} + \lambda_2\ket{v_2} + ... + \lambda_n\ket{v_n}$ for some $\lambda_1, \lambda_2, ... , \lambda_k\in\C^n$.
\end{definition}

\begin{remark}
If $\ket{w}$ is a linear combination of $\lbrace\ket{v_1}, ..., \ket{v_n}\rbrace\subset\C^n$, we can say that it belongs to the \textbf{span} of the set of $\lbrace\ket{v_1}, ..., \ket{v_n}\rbrace\subset\C^n$.
\end{remark}

\begin{example}
Create a linear combination of $\ket{v_1}=\bmat{i \\ 2}, \ket{v_2}=\bmat{-1 \\ i+1}$
\begin{center}
(\textit{foil}) \\
$(3+2i)\bmat{i \\ 2} + (2+i)\bmat{-1 \\ i+1}$ \\
(\textit{add}) \\
$=\bmat{3i-2 \\ 6+4i} + \bmat{-2-i \\ 3i+1}$ \\
$=\bmat{2i-4 \\ 7i+7}=\ket{w}$ \textit{spans} $\lbrace\ket{v_1}, \ket{v_2}\rbrace$
\end{center}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Linearly Independent %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Linearly Independent}
Another important concept is that of a set being \textit{linearly independent}. Being linearly independent essentially means that every piece of 'information' given by a set of vectors adds some sort of new information/perspective on the problem.

\begin{remark}
Two vectors are \textbf{linearly independent} as long as they are not \textit{parallel}.
\end{remark}

A good way of looking at this is that a system that is not linearly independent has more than one element that is some offset of the same constant. These elements are not giving new perspective, because they give the same information.

\begin{definition}\label{â€¢}
A set of vectors $\setovecs{v}$ is linearly independent if no vector is a linear combination of any other vectors. Algebraically, $[if \lambda_1\ket{v_1} + \lambda_2\ket{v_2} + ... + \lambda_n\ket{v_k}=\ket{0}, then \lambda_1=\lambda_2=\lambda_k=0]$
\end{definition}

The only way that the zero vector ($\ket{0}$) can be possible is if the complex constants ($\lambda$) are all the same, \textit{and} zero.

\begin{theorem}
If a set of $k$ vectors in $\C^n$ is linearly independent, then $k \leq n$. Equivalently, if you have a set of $k$ vectors in $\C^n$ such that $k>n$, then the set is linearly independent.
\end{theorem}

This theorem is important because it establishes the notion that a set can't contain more vectors than the space allows for. Said differently, the dimension of $M$ is the number of vectors contained within that are linearly independent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Basis %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Basis}
\begin{theorem}
A \textbf{basis} of a subspace $M\subseteq\C^n$ is a set of vectors such that
\begin{enumerate}
	\item $S$ is linearly independent
	\item Span($S$) $=M$ $\Longrightarrow$ "S Spans M"
\end{enumerate}
\end{theorem}
It follows that the standard basis of $\C^N$ is $\bmat{1 \\ 0 \\ 0 \\ ... \\ 0},\bmat{0 \\ 1 \\ 0 \\ ... \\ 0},\bmat{0 \\ 0 \\ 1 \\ ... \\ 0},\bmat{0 \\ 0 \\ 0 \\ ... \\ 1}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exercise 1 %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exercise 1}
Consider $S={\ket{v_1}, \ket{v_2}, \ket{v_3}}$ where $\ket{v_1}=\bmat{1 \\ -1}, \ket{v_2}=\bmat{i \\ 1}, \ket{v_3}=\bmat{0 \\ i}$
\begin{enumerate}[label=(\alph*)]
	\item Give a linear combination of the vectors in $S$. \\
	\begin{center}
	$\alpha\bmat{1 \\ -1} + \beta\bmat{i \\ 1} + \gamma\bmat{0 \\ i}=\ket{w}$ \\
	\end{center}
	Note that the next problem asks to determine if a vector is in span($S$). To 'kill two birds with 		one stone', try to find a linear combination that 			satisfies the question. To do this, assign 			values for $\alpha$, $\beta$, and $\gamma$. Note that $1+i=i+i$ (the top row), and $-1+1=0$. This
	allows us to set $\alpha$ and $\beta$ to $1$ and operate on $\gamma$.
	\begin{center}
	$i(x+yi)=200-i$ \\
	$xi-y$ $\Longrightarrow$ $x=-1$ $\Longrightarrow$ $\gamma=-1i-200$
	\end{center}
	Now plug in values.
	$1*\bmat{1 \\ -1} + 1*\bmat{i \\ 1} + (-200-1i)*\bmat{0 \\ i}=\ket{w}$
	\item Determine if $\bmat{1+i \\ 200-i}$ in Span($S$) \\
	Start with where we left off in the last part.
	\begin{center}
	$1*\bmat{1 \\ -1} + 1*\bmat{i \\ 1} + (-200-1i)*\bmat{0 \\ i}=\bmat{1+i \\ 200-i}$
	\end{center}
	$\bmat{1+i \\ 200-i}$ \textit{is} in Span($S$) because there existed values $\alpha$, $\beta$, and 	$\gamma$ such that $\ket{w}$ of $S$ is a possible outcome of $S$.
	\item Describe Span($S$) "geometrically" \\
	Span($S$) can be thought of as every possible vector $(\alpha\bmat{1 \\ -1} + \beta\bmat{i \\ 1} + 	\gamma\bmat{0 \\ i})$ contained within $\C^2$
	
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exercise 2 %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exercise 2}
Find the condition under which the following two vectors are linearly independent:
\begin{center}
$\ket{v_1}=\bmat{x \\ y \\ 3},\ket{v_2}=\bmat{2 \\ x-y \\ 1}\in\R^3$
\end{center}
Multiply $\ket{v_2}$ by a scalar that makes $\ket{v_1}$ and $\ket{v_2}$ \textit{not} linearly independent. This way, we can build a contradiction.
\begin{center}
$\ket{v_1}=\bmat{x \\ y \\ 3} = 3*\ket{v_2}=\bmat{2 \\ x-y \\ 1}$ \\
$\bmat{x \\ y \\ 3}=\bmat{6 \\ 3*(x-y) \\ 3}$
\end{center}
It can be seen that in this case, $\Longrightarrow x=6$ and $y=4.5$. \\
Therefore, as long as $x\neq6$ and $y\neq4.5$, $\ket{v_1}$ and $\ket{v_2}$ are linearly independent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exercise 3 %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exercise 3}
Show that the set formed by the following vectors is a basis for $\C^3$
\begin{center}
$\ket{v_1}=\bmat{1 \\ 1 \\ 1}, \ket{v_2}=\bmat{1 \\ 0 \\ 1}, \ket{v_3}=\bmat{1 \\ -1 \\ -1}$
\end{center}
Recall that for the vectors to be a basis for $\C^3$, they must be linearly independent, and
Span($\ket{v_1},\ket{v_2},\ket{v_3}$)$=\C^3$ (every vector must give additional information, IE no redundancy). To prove linear independence, we need to find where the constants ($\lambda$) are zero, by \ref{def:li}. \\
\begin{center}
\textit{Goal is to find Eigen values }
$\bmat{1 & 1 & 1 & 0 \\ 1 & 0 & -1 & 0 \\ 1 & 1 & -1 & 0}\longrightarrow (R_2-R_3)\longrightarrow\bmat{1 & 1 & 1 & 0 \\ 0 & -1 & 0 & 0 \\ 1 & 1 & -1 & 0}\longrightarrow (R_3-R_1)\longrightarrow\bmat{1 & 1 & 1 & 0 \\ 0 & -1 & 0 & 0 \\ 0 & 0 & -2 & 0}$ \\
Note that with linear algebra experience, this stage is enough to prove that we have linear independence. At the time of writing, I haven't taken this class so I'm going to continue. \\
(\textit{multiply by constant})
$\longrightarrow (-1R_2-\frac{1}{2}R_3)\longrightarrow\bmat{1 & 1 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0}=\bmat{1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0}$
\end{center}
To prove that the vectors provided are a basis for $\C^3$, we determined that the vectors are linearly independent such that $\alpha$, $\beta$, and $\gamma$ are zero. Then, noting that we're operating in $\C^3$, the vectors must be a basis. \textit{This is a bad way of explaining this, go to office hours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%
% Inner Products
%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inner Products}

Recall that $\ket{v}$ is a column vector and $\bra{v}$ is a row vector. Before talking about inner products, we must define operations that help the inner product process.
\begin{remark}
$(\C^n)^*$ is the "dual space of $\C^n$.
\end{remark}
The dual space is the space of all row vectors. This is useful for the next definition, which is critical in completing inner products.
\begin{definition}
We equip $\C^n$ with an "involution" (\textit{conjugate transpose}) operation:
\begin{center}
$\dagger:\C^n\longrightarrow(\C^n)^*$ given by $\bmat{x_1 \\ ... \\ x_n}=\bmat{x_1^* & ... & x_n^*}$ given a $\ket{v}\in\C^n$, $\bra{v}=\ket{v}^\dagger$
\end{center}
\end{definition}
A quick example of this operation is helpful to explain the significance of these definitions.
\begin{example}
Given $\ket{v}=\bmat{7 \\ 8i \\ \pi + 3i \\ 0}\in\C^4$, find $\ket{v}^\dagger$\\
Take the complex conjugate of each entry \\
$\bra{v}=\bmat{7 & -8i & \pi-3i & 0}$
\end{example}
Now, we can talk about inner products. Of primary importance for this section, the inner product can determine if two vectors are orthogonal, and operates the same as a dot product on $\R^n$.
\begin{definition}
Given $\ket{v}, \ket{w}\in\C^n$, the \textbf{inner product} of $\ket{v}$ and $\ket{w}$ is $\langle w|v \rangle=\bra{w}^\dagger\cdot\ket{v}$
\end{definition}
\begin{remark}
The inner product on $\R^n$ is the usual dot product.
\end{remark}
The following is a simple example showing the inner product on simple vectors in $\R^3$.
\begin{example}
Given $\ket{v}=\bmat{1 \\ 1\\ 3}, \ket{w}=\bmat{5 \\ -2 \\ 1}\in\R^3$, find $\langle w|v \rangle$ \\
\indent Perform a complex conjugate on $\ket{w}$ (In $\R^3$, this is the same as flipping $nx1$ to $1xn$) \\
\indent $\langle w|v \rangle=\bmat{5 & -2 & 1}\cdot\bmat{1 \\ 1 \\ 3}=(5*1)+(-2*1)+(1*3)=6$
\end{example}
Now, the following example shows the inner product on two simple vectors in $\C^2$.
\begin{example}\label{ex:cpcmp}
Given $\ket{v}=\bmat{i \\ 1}, \ket{w}=\bmat{i \\ -1}\in\C^2$, find $\inp{w}{v}$
\begin{center}
$\inp{w}{v}=\bmat{-i & -1}\cdot\bmat{i \\ 1}=1+(-1*1)=0$
\end{center}
\end{example}
Recall from calculus III that the dot product of two vectors can give us insight on orthogonality. In the case of example \ref{ex:cpcmp}, the inner product of $\langle w|v \rangle$ was zero. This tells us that the two vectors are orthogonal.
\begin{definition}
The \textbf{norm} (similar to magnitude) of $\ket{v}\in\C^n$ is $\norm{\ket{v}}:=\sqrt{\inp{w}{v}}$ 
\end{definition}
\begin{example}
$\ket{v}=\bmat{1 \\ 1}\in\R^2\longrightarrow||\ket{v}||=\sqrt{1^2+1^2}=\sqrt{2}$
\end{example}
\begin{remark}
\begin{itemize}
	\item $\ket{v}\in\C^n$ with $||\ket{v}||=1$ is called a unit vector
	\item Unit vectors in $\C^n$ represent a \textit{quantum state}
	\item An important property of a norm is the \textit{triangle inequality} $\longrightarrow\norm{\ket{v}+\ket{w}}\leq\norm{\ket{v}}+\norm{\ket{w}}$
\end{itemize}
\end{remark}
\begin{definition}
A set of non-zero vectors $S=\setofkets{v}\leq\C^n$ is called an orthogonal set if $\inp{v}{w}=0$ if $i\neq j$
\end{definition}
\begin{theorem}
If $S=\setofkets{v}$ is an orthogonal set of non-zero vectors in $\C^n$, then $S$ is linearly independent.
\end{theorem}
\begin{proof}
Let $c_1, ..., c_k\in\C$ such that $c_1\ket{v_1}+...+c_k\ket{v_k}$. \\
\textit{Goal: show that $c_1=c_2=c_k=0\longrightarrow$ linearly independent using $S$ is orthogonal} \\
Additionally, let \\ 
\indent $j\in{1,...,k}$.\\
Then, \\ 
\indent $\bra{v_j}(c_1\ket{v_1}+...+c_k)=\inp{v_j}{0}$ \\
\indent $\longrightarrow c_1\inp{v_j}{v_1}+...+c_k\inp{v_j}{v_k}$ \\
\indent $c_j\inp{v_j}{v_j}=$ (\textit{$v_j$ is the only component 'allowed' to have magnitude}) \\
Thus, $c_j=0$ since $\ket{v_j}\neq\ket{0}$ \\
Note: Since $j$ was arbitrary, $c_1 = c_2 = ... = c_k = 0$. Hence, $S$ is linearly independent.
\end{proof}

\begin{remark}
$S$ is an orthonormal basis if:
\begin{itemize}
	\item $S$ is an orthonormal set if $n$ unit vectors
	\item $\spn{S}=\C^n$
\end{itemize}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Projections %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Projections}

\begin{definition}
Define $\ket{f_i}\bra{f_i}$ to be the \textbf{projection} operator onto $\spn{\ket{f_i}}$
\end{definition}

\begin{theorem}
Let $\beta=\setofkets{f}$ be an orthonormal basis for $\C^n$. Then, any $\ket{x}=c_1\ket{f_1}+...+c_n\ket{f_n}=\inp{f_1}{x}\ket{f_1}+...+\inp{f_n}{x}\ket{f_n}$.
\end{theorem}
It follows that $\sum_{j=1}^{n} \ket{f_j}\bra{f_j}=\bmat{1 &  &  \\  & ... &  \\  &  &  1}$

\begin{example}
Let $\ket{f_1}=\frac{-2}{\sqrt{2}}\bmat{1 \\ 1},\ket{x}=\bmat{-2 \\ 1}$. \\
$\ket{x_|}=\inp{f_1}{x}\ket{f_1}=P_1(\ket{x}$ \\
$P_1(\ket{x})=\ket{f_1}\inp{f_1}{x}$
\end{example}
In the above example, $\ket{x_|}$ is the projection of $x$ onto $f_1$ in the direction of $f_1$.
Note that the projection operation is similar to \textit{comp} from calculus III. Now, let's look at some properties of the projection operator.
\begin{proposition}
Let $\beta=\setofkets{f}$ be an orthonormal basis for $\C^n$. Then, $\lbrace P_1, P_2, P_n\rbrace$ is a set of $n\times n$ matrices such that
\begin{enumerate}
\item $P_i(\ket{v})\in\spn(\ket{f_i}$
\item $\ket{v}-P_i(\ket{v})$ is orthogonal to $\ket{f_i}$
\item $P_i^2=P_i*P_i=P_i$
\item $P_iP_j=0$ when $i\neq\j$, and $P_i^\dagger=P_i$
\item $\sum_{i=1}^{n} P_i=I_n$ IE the sum of $P$'s gives us the identity matrix.
\end{enumerate}
\end{proposition}

Projection operators make it easy for us to find $c_1, c_2, c_n$ for $\ket{x}=c_1\ket{f_1}+...+c_k\ket{f_k}$ when $\setofkets{f}$ is an orthonormal set. Side note, in quantum land, $P_i$'s are how we measure the probability that $\ket{x}$ is actually in $\ket{f_i}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Gram-Schmidt %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gram-Schmidt}

Let $\beta=\setofkets{b}$ be a basis for $\C^n$
\begin{itemize}
\item Linearly independent
\item $\spn{\beta}=\C^n$ (IE no redundancy)
\end{itemize}
Goal: Turn $\beta$ into an orthonormal basis, $\setofkets{f}$
\begin{enumerate}
\item First, make an orthogonal basis. \\
$\ket{f_{i+1}}:=\ket{b_{i+1}}-(\sum_{j=1}{i} \frac{\inp{f_j}{b_{i+1}}}{\inp{f_j}{f_j}}\ket{f_j})$
\item Then, make an orthonormal basis by normalizing the resultant ($\ket{f}$) vectors. \\
For each $\ket{f}$, $\ket{f}=\frac{1}{\norm{\ket{f}}}\ket{f}$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exercise 7 %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exercise 7}
Let $\beta=\lbrace\ket{b_1},\ket{b_2}\rbrace$, where $\ket{b_1}=\frac{1}{\sqrt{2}}\bmat{1 \\ 1}, \ket{b_2}=\frac{1}{\sqrt{2}}\bmat{1 \\ -1}$
\begin{enumerate}[label=(\alph*)]
	\item Show that $\beta$ is an orthonormal basis for $\C^2$ \\
	\begin{enumerate}[label=(\roman*)]
		\item Show that they're orthogonal
		\begin{center}
		$\inp{w}{v}=\bra{w}^\dagger\cdot\ket{v}$ \\
		$\frac{1}{\sqrt{2}}(\bmat{1 & 1}\bmat{1 \\ -1}$ \\
		$=\frac{1}{\sqrt{2}}((1*1)+(1*-1))=0\longleftarrow$ Recall that if the inner product is zero, the 					vectors are orthogonal
		\end{center}
		\item Determine if $\ket{b_1}$ and $\ket{b_2}$ are unit vectors
		\begin{center}
		\indent $\frac{1}{\sqrt{2}}\sqrt{1^2+1^2}=\frac{\sqrt{2}}{\sqrt{2}}=1\longleftarrow$ Unit vector
		\end{center}
		Therefore, $\beta$ is an orthonormal basis.
	\end{enumerate}
	\item Find the coordinates (or components) of $\ket{x}=\bmat{-2 \\ 1}$ relative to $\beta$. IE find the 			scalars $c_1, c_2 \in \C$ such that $c_1\ket{b_1}+c_2\ket{b_2}=\ket{x}$.
	\begin{enumerate}[label=(\roman*)]
		\item Multiply both sides by $\bra{b_1}$
		\begin{center}
		$\bra{b_1}(c_1\ket{b_1}+c_2\ket{b_2})=\inp{b_1}{x}$ \\
		$c_1\inp{b_1}{b_1}+c_2\inp{b_1}{b_2} = c_1*1+c_2*0=c_1$ \\ 
		Note that inner product of itself is parallel, and since $\beta$ is an orthonormal basis, $\inp{b_1}{b_2}			=1$ (IE they're orthogonal). \\
		$c_1=\inp{b_1}{x}$ \\
		$c_1=\frac{1}{\sqrt{2}}\bmat{1 & 1}\bmat{-2 \\ 1}\longrightarrow =\bmat{\frac{1}{\sqrt{2}} & \frac{1}				{\sqrt{2}}}\bmat{-2 \\ 1}$ \\
		$\frac{-2}{\sqrt{2}}+\frac{1}{\sqrt{2}}=\frac{-1}{\sqrt{2}}$ \\
		\end{center}
		\item Multiply both sides by $\bra{b_2}$
		\begin{center}
		$\bra{b_2}(c_1\ket{b_1}+c_2\ket{b_2})=\inp{b_2}{x}$ \\
		$c_1\inp{b_2}{b_1}+c_2\inp{b_2}{b_2} = c_1*0+c_2*1=c_2$ \\
		$c_2=\inp{b_2}{x}$ \\
		
		$c_2=\frac{1}{\sqrt{2}}\bmat{1 & -1}\bmat{-2 \\ 1}\longrightarrow =\bmat{\frac{1}{\sqrt{2}} & \frac{-1}				{\sqrt{2}}}\bmat{-2 \\ 1}$ \\
		$\frac{-2}{\sqrt{2}}+\frac{-1}{\sqrt{2}}=\frac{-3}{\sqrt{2}}$ \\
		\end{center}
	\end{enumerate}
	Therefore, $c_1=\frac{-1}{\sqrt{2}}$ and $c_2=\frac{-3}{\sqrt{2}}$ when $\ket{x}=\bmat{-2 \\ 1}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Exercise (Gram-Schmidt) %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exercise (Gram-Schmidt)}
Given $S=\rbrace\ket{v_1}=\bmat{-1 \\ 2 \\ 2}, \ket{v_2}=\bmat{2 \\ -1 \\ 2}, \ket{v_3}=\bmat{3 \\ 0 \\ 3}\lbrace$, turn $S$ into an orthonormal basis for $\spn{S}=\R^3$.
\begin{enumerate}
\item Find $\ket{f_1}$
\begin{center}
$\norm{\ket{v_1}}=3$ \\
$\longrightarrow\ket{f_1}=\frac{1}{3}\bmat{-1 \\ 2 \\ 2}$ \\
\end{center}
\item Find $\ket{f_2}$
\begin{center}
$\ket{f_2}=\ket{v_2}-\left(\frac{\inp{v_1}{v_2}}{\inp{v_1}{v_1}}\ket{v_1}\right)$ \\
$\bmat{2 \\ -1 \\ 2}-\left[ \bmat{-1 & 2 & 2} \bmat{2 \\ -1 \\ 2} \right] =\bmat{2 \\ -1 \\ 2}-\ket{0}$ \\
$\ket{f_2}=\frac{1}{3}\bmat{2 \\ -1 \\ 2}$
\end{center}
\item Find $\ket{f_3}$
\begin{center}
$\ket{f_3}=\ket{v_3}-\left[ \left( \frac{\inp{v_1}{v_3}}{\inp{v_1}{v_1}}\ket{v_1} \right) + \left( \frac{\inp{v_2}{v_3}}{\inp{v_2}{v_2}}\ket{v_2} \right)  \right]$ \\
$=\bmat{3 \\ 0 \\ 3}-\left[ \left( \frac{\bmat{-1 & 2 & 2}\bmat{3 \\ 0 \\ 3}}{\bmat{-1 & 2 & 2}\bmat{-1 \\ 2 \\ 2}} \right) + \left( \frac{\bmat{2 & -1 & 2}\bmat{3 \\ 0 \\ 3}}{\bmat{2 & -1 & 2}\bmat{2 \\ -1 \\ 2}} \right) \right]$ \\
$=\bmat{3 \\ 0 \\ 3}-\left( \bmat{\frac{-1}{3} \\ \frac{2}{3} \\ \frac{2}{3}} + \bmat{\frac{8}{3} \\ \frac{-4}{3} \\ \frac{8}{3}} \right) =\bmat{\frac{2}{3} \\ \frac{2}{3} \\ \frac{-1}{3}}$ \\
$\norm{f_3}=1$ \textit{already normalized}
\end{center}
\end{enumerate}
Therefore, $\ket{f_{final}}=\frac{1}{3}\bmat{-1 \\ 2 \\ 2}, \frac{1}{3}\bmat{2 \\ -1 \\ 2}, \bmat{\frac{2}{3} \\ \frac{2}{3} \\ \frac{-1}{3}}$

\section{Matrices and Linear Transformations}
Lets goooo
\begin{definition}
A map $T: \C^n\longrightarrow\C^m$ is a linear transformation if:
\begin{enumerate}
	\item $T(\ket{v}+\ket{u}=T(\ket{v}|\ket{u})$
	\item $T(C\mid V)=CT(\ket{v}) \forall \ket{u},\ket{v}\in\C^n$
\end{enumerate}
\end{definition}
Note that every linear transformation arises from a matrix.
\begin{definition}
$M_{mn}(\C)$ is a set of all $m\times n$ matrices with complex entries $M_N(\C):=M_{nm}(\C)$
\end{definition}
This is traditionally written as $M_n$ or $M_{mn}$. $M_{mn}$ is a vector space with usual scalar multiplication and matrix addition.

\begin{example}
Given $\sigma_x=\bmat{0 & 1 \\ 1 & 0}$, $\sigma_y=\bmat{0 & -i \\ i & 0}\in M_2$, $\sigma_z=\bmat{1 & 0 \\ 0 & -1}$, find $2\omega_x-i\omega_y$
\begin{center}
$=\bmat{0 & 1 \\ 1 & 0}-i\bmat{0 & -i \\ i & 0}$ \\
$=\bmat{0 & 2 \\ 2 & 0}-\bmat{0 & 1 \\ -1 & 0}$ \\
$=\bmat{0 & 1 \\ 3 & 0}$
\end{center}
\end{example}
Note that $\sigma_x,\sigma_y,\sigma_z$ are called Pauli or 'spin' matrices. \\
$I_n$ is called the identity matrix whose $0$'s are the standard basis for $\setofkets{e}\in\C^n$. IE: $I_N=\bmat{1 & 0 & 0 & ... & 0 \\ 0 & 1 & 0 & ... & 0 \\ 0 & 0 & 1 & ... & 0 \\ ... & ... & ... & ... & ... \\ 0 & 0 & 0 & ... & 1}$ $M_N$ has well-defined multiplication of matrices. \\ In general, if $A\in M_{mn}$, $B=m_{nk}$, $A=(a_{ij})^{mn}_{j=1 j=1}$, $B=(b_{rs})_{r=1, 2=1}$. Additionally, the matrix $AB=\bmat{A\ket{b_1} & ... & B\ket{b_k}}=(C_{pq})$ where $C_{pq}$ is the dot product of the $p$th row of $A$ against the $q$th column of $B$. \\
Here are some nice facts that will help us in further examples:
\begin{enumerate}
\item $AB=\bmat{\bra{a_1}B \\ ... \\ \bra{a_m}B}$ where $\setofkets{a}$ are the rows of $A$.
\item $AB=\sum^n_{i=1}\ket{a_j}\bra{b_j}$ (IE "the columns of $A$ against the rows of $B$")
\end{enumerate}
It's useful to know some matrix operations to proceed. In general, $\bmat{\ket{a_1} & ... & \ket{a_n}}\bmat{d_1 & & 0 \\ & ... & \\ 0 & & d_n}=\bmat{d_1\ket{a_1} & ... & d_n\ket{a_n}}$ Here's an example:
\begin{example}
\begin{center}
$\bmat{0 & -i \\ i & 0}\bmat{1 & 0 \\ 0 & 3}=\bmat{0 & -3i \\ i & 0}$
\end{center}
\end{example}
\begin{definition}
An \textbf{eigenvalue} for $A\in M_n$ is a complex number $\lambda\in\C^n$ such that there is a non zero vector $\ket{x}\in\C^n$ satisfying $A\ket{x}=\lambda\ket{x}$.
\end{definition}
Said differently, the eigenvalue for A is somehow correlated to $\lambda$.
\begin{definition}
A matrix $A\in M_n$ is diagonalizable if:
\begin{enumerate}
\item There is a diagonal matrix $D$ and an invertable  matrix $P$ such that $A=PDP^-1$.
\item There exists a basis for $\C^n$ consisting of eigenvectors for A
\end{enumerate}
\end{definition}

\begin{example}
Consider $A\bmat{I_2 & 0 \\ 0 & \sigma_y}$ \\ First, let's get the eigenvalues and normalized vectors. Note that eigenvalues are the \textit{roots} of the characteristic equation $det(A-\lambda I)=0$
\begin{center}
$\bmat{1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 &-i \\ 0 & 0 & i & 0}-\bmat{\lambda & & & 0 \\ & \lambda & & & \\ & & \lambda & & \\ 0 & & & \lambda &}$ \\
$A-\lambda I=\bmat{1 - \lambda & 0 & 0 & 0 \\ 0 & 1-\lambda & 0 & 0 \\ 0 & 0 & -\lambda & -i \\ 0 & 0 & i & \lambda}\longrightarrow det(A-\lambda I)=$
$\begin{vmatrix}
1 - \lambda & 0 & 0 & 0 \\ 
0 & 1-\lambda & 0 & 0 \\ 
0 & 0 & -\lambda & -i \\ 
0 & 0 & i & \lambda \notag
\end{vmatrix}$ \\
$=(1-\lambda)\begin{vmatrix}
1-\lambda & 0 & 0 \\ 
0 & -\lambda & -i \\ 
0 & i & \lambda \notag
\end{vmatrix}=(1-\lambda)\bmat{1-\lambda & \bmat{-\lambda & -i \\ i & \lambda} & -0 & 0}$ \\
$=(1-\lambda)(1-\lambda)\bmat{\lambda^2 & -1}=0$ \\
$\lambda=1,1,1,-1$ \\
These are our eigenvalues. Now, we need to find our eigenvectors\\
Note: $A\ket{e_1}=\ket{e_1}$ and $B\ket{e_2}=\ket{e_2}$. \\
$A-(1)I=\bmat{0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & -1 & -i \\ 0 & 0 & i & 1}\xrightarrow{R_4+iR_3}\bmat{0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & -1 & i \\ 0 & 0 & 0 & 0}\xrightarrow[-1*R_1]{R_1\leftrightarrow R_3}=\bmat{0 & 0 & 1 & -i \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0}$ \\
Therefore, our eigen vector is $\ket{x}=\bmat{0 \\ 0 \\ i \\ 1}$. \\Our normalized eigen vectors are $\left\lbrace\bmat{1 \\ 0 \\ 0 \\ 0},\bmat{0 \\ 1 \\ 0 \\ 0},\frac{1}{\sqrt{2}}\bmat{0 \\ 0 \\ i \\ 1},\frac{1}{\sqrt{2}}\bmat{0 \\ 0 \\ 1 \\ i}\right\rbrace $
\end{center}
\end{example}

\subsection{Subsection of something}
Some, but not all matrices are diagonalizable; this property is highly desirable. Matrices are diagonalizable if and only if the eigenvalues form a basis. \\ \textit{put some shit here} \\
\begin{enumerate}
\item Vectors must be self-adjoint (\textit{hermition}) IE $A\in M_n$ is \textbf{hermition} if $A^\dagger=A$, $sigma+\dagger_x=\sigma_x$, $\sigma+\dagger_y=sigma_y$, $\sigma+\dagger_z=\sigma_z$
\item As it turns out, \textit{all} hermition matricies are diagonalizable $\longleftrightarrow$ the eigenvectors for a basis (IE linearly independent and Span). Said differently, hermition matrices are only hermition matrices if their eigenvectors form an orthonormal basis (IE unit vector, norm, and Span). It's worth noting that the eigenvalues must be real for this to be the case.
\item A matrix in $M_n$ is positive-semidefinite if for all $\ket{x}\in\C^n$, we have $\bra{x}A\ket{x}\geq0$
\item The following are equivalent:
\begin{enumerate}
\item $u\in M_n$ is unitary
\item $u^\dagger=u-1$ (IE left \textit{and} right inverse) $\longrightarrow u^{-1}u=uu^{-1}$
\item $uu^\dagger=I_n$ and $u^\dagger u=I_n$ (unitaries are normal, but not necessarily hermition)
\item The columns of $u$ form an orthonormal basis for $\C^n$
\item $\forall\ket{x},\ket{y}\in\C^n$, $\inp{u_x}{u_y}=\inp{x}{y}$. Think of this as a rotation of the entire matrix, such that the angle between the vectors are preserved. Related to calculus III, this operation gives us the angle.
\end{enumerate}
\end{enumerate}
Now, let's go over some examples of these properties in action.
\begin{example}
Example of a matrix that is \textit{not} Hermition:
\begin{center}
$N=\bmat{2 & -i \\ -i & 2}\longrightarrow N^\dagger=\bmat{2 & i \\ i & 2}$
\end{center} Note: the columns of $N$ are not orthogonal, so $N$ is not unitary.
\end{example}
\begin{example}
Example of a unitary matrix that's not Hermition: 
\begin{center}
$\bmat{\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}}\longrightarrow u^{-1}=u^\dagger=\bmat{\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}}$
\end{center}
\end{example}
\begin{example}
$\forall\ket{x}\in{C^n}$, $\bra{x}A\ket{x}\geq 0$: $A=\ket{e_2}\bra{e_2}$ where $e_2=\bmat{0 \\ 1}=\bmat{0 & 0 \\ 0 & 1}$ This is positive semidefinite if: \\
\textit{proof} Let $\ket{x}=\bmat{a \\ b}\in\C^2$. \\
$\bra{x}A\ket{x}=\bmat{a^* & b^*}\bmat{0 & 0 \\ 0 & 1}\bmat{a \\ b}$ \\
$\bmat{a^* & b^*}\bmat{0 \\ b}=0+b^*b=\mid b\mid^2\geq0$
\end{example}
\begin{theorem}
A matrix $N\in M_n$ is normal $\longleftrightarrow$ there is a unitary matrix $u=\bmat{\ket{u_1} & ... & \ket{u_n}}$ and a diagonalizable matrix $D=\bmat{\lambda & 0 \\ 0 & \lambda_n}$ such that $N=uDu^\dagger$
\end{theorem}
To perform decomposition of matrices, we can use the \textit{spectral theorem for normal matrices}, and the fact that in general, $A\in M_n$ has a singular value decomposition. Note that both of these use eigenvectors of the given matrix.

%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% end of document
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\end{document}